#!/usr/bin/env python3
"""
í‚¤ì›Œë“œ ë°€ë„ ë° ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë¸”ë¡œê·¸ ì½˜í…ì¸ ì˜ í‚¤ì›Œë“œ ë°€ë„ì™€ ë¶„í¬ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python3 scripts/test_keyword_analysis.py [project_id]

ì˜ˆì‹œ:
    python3 scripts/test_keyword_analysis.py 20260207
    python3 scripts/test_keyword_analysis.py  # ê¸°ë³¸ê°’: 20260207 ì‚¬ìš©
"""

import sys
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€ (scripts/ ì•„ë˜ë¡œ ì´ë™í–ˆê¸° ë•Œë¬¸)
PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))
os.chdir(PROJECT_ROOT)

try:
    from src.quality.keyword_analyzer import KeywordDensityAnalyzer
    MODULES_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Some modules not available: {e}")
    MODULES_AVAILABLE = False


class KeywordAnalysisTester:
    """í‚¤ì›Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self, project_id: str):
        self.project_id = project_id
        self.project_dir = Path(f"data/{project_id}")
        self.generated_blog_path = self.project_dir / "generated_blog.txt"
        self.keyword_report_path = self.project_dir / "keyword_analysis_report.json"
        self.logs_dir = self.project_dir / "logs"

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.logs_dir.mkdir(parents=True, exist_ok=True)

    def log_event(self, stage: str, status: str, message: str, **kwargs):
        """êµ¬ì¡°í™”ëœ ë¡œê·¸ ê¸°ë¡"""
        timestamp = datetime.now().isoformat()
        log_entry = {
            "timestamp": timestamp,
            "stage": stage,
            "status": status,
            "message": message,
            **kwargs
        }

        # ì½˜ì†” ì¶œë ¥
        print(f"[{timestamp}] {stage.upper()} - {status}: {message}")
        for key, value in kwargs.items():
            if value is not None:
                print(f"  â””â”€ {key}: {value}")

        # JSON ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡
        log_file = self.logs_dir / "keyword_analysis_log.json"

        if log_file.exists():
            with open(log_file, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        else:
            logs = []

        logs.append(log_entry)

        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=2, default=str)

    def check_prerequisites(self) -> bool:
        """ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        self.log_event("prerequisites", "info", "ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì‹œì‘")

        # 1. ëª¨ë“ˆ ê°€ìš©ì„± í™•ì¸
        if not MODULES_AVAILABLE:
            self.log_event("prerequisites", "failed", "í•„ìš” ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False

        # 2. ìƒì„±ëœ ë¸”ë¡œê·¸ ê¸€ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not self.generated_blog_path.exists():
            self.log_event("prerequisites", "failed", f"ë¸”ë¡œê·¸ ê¸€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.generated_blog_path}")
            return False

        file_size = self.generated_blog_path.stat().st_size
        self.log_event("prerequisites", "completed", "ëª¨ë“  ì‚¬ì „ ìš”êµ¬ì‚¬í•­ì´ ì¶©ì¡±ë˜ì—ˆìŠµë‹ˆë‹¤",
                      blog_file_size=file_size)
        return True

    def load_blog_content(self) -> Optional[str]:
        """ìƒì„±ëœ ë¸”ë¡œê·¸ ê¸€ ë¡œë“œ"""
        self.log_event("content_loading", "info", f"ë¸”ë¡œê·¸ ê¸€ ë¡œë“œ ì¤‘: {self.generated_blog_path}")

        try:
            with open(self.generated_blog_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            self.log_event("content_loading", "completed", "ì½˜í…ì¸  ë¡œë“œ ì„±ê³µ",
                          content_length=len(content),
                          word_count=len(content.split()))

            return content

        except Exception as e:
            self.log_event("content_loading", "failed", f"ì½˜í…ì¸  ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None

    def extract_hashtags_as_keywords(self, content: str) -> Optional[list]:
        """í•´ì‹œíƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        hashtags = re.findall(r'#([ê°€-í£a-zA-Z0-9_]+)', content)

        # í•œê¸€ í•´ì‹œíƒœê·¸ë§Œ ì¶”ì¶œí•˜ê³  ì •ë¦¬
        korean_keywords = []
        for tag in hashtags:
            # í•œê¸€ì´ í¬í•¨ëœ íƒœê·¸ë§Œ ì„ íƒ
            if re.search(r'[ê°€-í£]', tag):
                # 'ë§›ì§‘', 'ì¹´í˜', 'ê°•ì¶”' ë“±ì˜ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
                clean_keyword = re.sub(r'[a-zA-Z0-9_]', '', tag)  # ì˜ë¬¸/ìˆ«ì ì œê±°
                if len(clean_keyword) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                    korean_keywords.append(clean_keyword)

        return korean_keywords if korean_keywords else None

    def run_keyword_analysis(self, content: str) -> Optional[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰"""
        self.log_event("keyword_analysis", "info", "í‚¤ì›Œë“œ ë°€ë„ ë¶„ì„ ì‹œì‘")

        try:
            # KeywordDensityAnalyzer ì´ˆê¸°í™”
            analyzer = KeywordDensityAnalyzer()

            # í•´ì‹œíƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            target_keywords = self.extract_hashtags_as_keywords(content)

            if target_keywords:
                self.log_event("keyword_extraction", "info", f"ëŒ€ìƒ í‚¤ì›Œë“œ ì¶”ì¶œ: {', '.join(target_keywords)}")
                # íŠ¹ì • í‚¤ì›Œë“œ ë¶„ì„
                analysis_result = analyzer.analyze_keyword_density(content, target_keywords)
            else:
                self.log_event("keyword_extraction", "info", "ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë“œ")
                # ìë™ í‚¤ì›Œë“œ ë¶„ì„
                analysis_result = analyzer.analyze_keyword_density(content)

            quality_score = analysis_result["quality_score"]

            self.log_event("keyword_analysis", "completed", "í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ",
                          overall_score=quality_score["overall_score"],
                          rating=quality_score["rating"],
                          passed=quality_score["passed"])

            return analysis_result

        except Exception as e:
            error_result = {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__,
                "timestamp": datetime.now().isoformat()
            }
            self.log_event("keyword_analysis", "failed", f"í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return error_result

    def save_keyword_report(self, analysis_result: Dict[str, Any], original_content: str):
        """í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        self.log_event("report_saving", "info", f"í‚¤ì›Œë“œ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ ì¤‘: {self.keyword_report_path}")

        try:
            # ì €ì¥ìš© ë°ì´í„° êµ¬ì„±
            report_data = {
                "project_id": self.project_id,
                "analysis_timestamp": datetime.now().isoformat(),
                "original_content": original_content,
                "analysis_result": analysis_result,
                "file_info": {
                    "blog_file": str(self.generated_blog_path),
                    "report_file": str(self.keyword_report_path),
                    "log_file": str(self.logs_dir / "keyword_analysis_log.json")
                }
            }

            with open(self.keyword_report_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)

            self.log_event("report_saving", "completed", "í‚¤ì›Œë“œ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ ì„±ê³µ")

        except Exception as e:
            self.log_event("report_saving", "failed", f"ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def display_results(self, analysis_result: Dict[str, Any]):
        """ê²°ê³¼ í™”ë©´ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ” í‚¤ì›Œë“œ ë°€ë„ ë° ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!")
        print("=" * 80)

        if "error" not in analysis_result:
            content_stats = analysis_result["content_stats"]
            quality_score = analysis_result["quality_score"]
            keyword_analysis = analysis_result["keyword_analysis"]
            density_evaluation = analysis_result["density_evaluation"]
            distribution_analysis = analysis_result["distribution_analysis"]

            # ì¢…í•© ê²°ê³¼
            print(f"\nğŸ“Š ì¢…í•© í‰ê°€:")
            print(f"   ğŸ¯ í’ˆì§ˆ ì ìˆ˜: {quality_score['overall_score']:.2f} ({quality_score['rating']})")
            print(f"   ğŸ“ˆ ë°€ë„ ì ìˆ˜: {quality_score['density_score']:.2f}")
            print(f"   ğŸ“Š ë¶„í¬ ì ìˆ˜: {quality_score['distribution_score']:.2f}")
            print(f"   âœ… í†µê³¼ ì—¬ë¶€: {'í†µê³¼' if quality_score['passed'] else 'ë¯¸í†µê³¼'}")

            # ì½˜í…ì¸  í†µê³„
            print(f"\nğŸ“ ì½˜í…ì¸  í†µê³„:")
            print(f"   ğŸ“„ ì´ ë‹¨ì–´ ìˆ˜: {content_stats['total_words']}ê°œ")
            print(f"   ğŸ”¤ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {content_stats['unique_words']}ê°œ")
            print(f"   ğŸ“Š ì–´íœ˜ ë‹¤ì–‘ì„±: {content_stats['lexical_diversity']:.1%}")
            print(f"   ğŸ“ ê¸€ì ìˆ˜: {content_stats['content_length']}ì")

            # í‚¤ì›Œë“œ ë¶„ì„
            keywords = keyword_analysis["keywords"]
            print(f"\nğŸ”‘ í‚¤ì›Œë“œ ë¶„ì„:")
            print(f"   ğŸ“‹ ë¶„ì„ ë°©ì‹: {'íƒ€ê²Ÿ í‚¤ì›Œë“œ' if keyword_analysis['method'] == 'target_keywords' else 'ìë™ ì¶”ì¶œ'}")
            print(f"   ğŸ” í‚¤ì›Œë“œ ìˆ˜: {len(keywords)}ê°œ")
            print(f"   ğŸ“Š í‰ê·  ë°€ë„: {keyword_analysis['average_density']:.1%}")

            # ìƒìœ„ í‚¤ì›Œë“œ ë°€ë„
            if keywords:
                print(f"   ğŸ“ˆ í‚¤ì›Œë“œë³„ ë°€ë„:")
                sorted_keywords = sorted(keywords.items(), key=lambda x: x[1]["density"], reverse=True)
                for i, (keyword, data) in enumerate(sorted_keywords[:5], 1):  # ìƒìœ„ 5ê°œ
                    risk_color = "ğŸ”´" if data["density"] > 0.03 else "ğŸŸ¡" if data["density"] > 0.02 else "ğŸŸ¢"
                    print(f"      {i}. {keyword}: {data['density']:.1%} ({data['count']}íšŒ) {risk_color}")

            # ë°€ë„ í‰ê°€
            print(f"\nâš ï¸  ë°€ë„ í‰ê°€:")
            print(f"   ğŸ”º ìµœê³  ë°€ë„: {density_evaluation['max_density']:.1%}")
            print(f"   ğŸ“Š í‰ê·  ë°€ë„: {density_evaluation['average_density']:.1%}")
            print(f"   âš ï¸  ìœ„í—˜ë„: {density_evaluation['overall_risk']}")

            # ë¶„í¬ ë¶„ì„
            print(f"\nğŸ“ ë¶„í¬ ë¶„ì„:")
            print(f"   ğŸ“Š íŒ¨í„´: {distribution_analysis['pattern']}")
            print(f"   âš–ï¸  ê· ë“±ì„±: {distribution_analysis['evenness']:.2f}")
            print(f"   ğŸ“ˆ ì»¤ë²„ë¦¬ì§€: {distribution_analysis['coverage_ratio']:.1%}")
            print(f"   ğŸ“ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥: {distribution_analysis['sentences_with_keywords']}/{distribution_analysis['total_sentences']}ê°œ")

            # ìœ„í—˜ í‚¤ì›Œë“œ
            high_risk_keywords = [
                keyword for keyword, eval_data in density_evaluation["keyword_evaluations"].items()
                if eval_data["risk_level"] in ["HIGH", "CRITICAL"]
            ]

            if high_risk_keywords:
                print(f"\nğŸš¨ ì£¼ì˜ í•„ìš” í‚¤ì›Œë“œ:")
                for keyword in high_risk_keywords:
                    eval_data = density_evaluation["keyword_evaluations"][keyword]
                    print(f"   â€¢ {keyword}: {eval_data['density']:.1%} ({eval_data['risk_level']}) - {eval_data['recommendation']}")

            # ê°œì„  ê¶Œì¥ì‚¬í•­
            recommendations = analysis_result["recommendations"]
            if recommendations:
                print(f"\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
                for i, rec in enumerate(recommendations, 1):
                    print(f"   {i}. {rec}")

            # ë‹¨ì–´ ë¹ˆë„ TOP 5
            word_frequency = analysis_result["word_frequency"]
            if word_frequency["most_frequent"]:
                print(f"\nğŸ“Š ìµœê³  ë¹ˆë„ ë‹¨ì–´ TOP 5:")
                for i, word_data in enumerate(word_frequency["most_frequent"][:5], 1):
                    print(f"   {i}. {word_data['word']}: {word_data['frequency']:.1%} ({word_data['count']}íšŒ)")

        else:
            print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨:")
            print(f"   ì˜¤ë¥˜ ìœ í˜•: {analysis_result.get('error_type', 'Unknown')}")
            print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {analysis_result.get('error', 'No message')}")

        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print(f"   - {self.keyword_report_path}")
        print(f"   - {self.logs_dir / 'keyword_analysis_log.json'}")

    def run_full_analysis(self):
        """ì „ì²´ í‚¤ì›Œë“œ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            print(f"\nğŸ” í‚¤ì›Œë“œ ë°€ë„ ë° ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
            print(f"   í”„ë¡œì íŠ¸ ID: {self.project_id}")
            print(f"   í”„ë¡œì íŠ¸ ê²½ë¡œ: {self.project_dir}")
            print("=" * 80)

            # 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸
            if not self.check_prerequisites():
                print("\nâŒ ì‚¬ì „ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ì§€ ì•Šì•„ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return

            # 2. ë¸”ë¡œê·¸ ì½˜í…ì¸  ë¡œë“œ
            content = self.load_blog_content()
            if not content:
                print("\nâŒ ë¸”ë¡œê·¸ ì½˜í…ì¸  ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return

            # 3. í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
            analysis_result = self.run_keyword_analysis(content)
            if not analysis_result:
                print("\nâŒ í‚¤ì›Œë“œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return

            # 4. ê²°ê³¼ ì €ì¥
            self.save_keyword_report(analysis_result, content)

            # 5. ê²°ê³¼ ì¶œë ¥
            self.display_results(analysis_result)

            self.log_event("full_process", "completed", "ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")

        except Exception as e:
            self.log_event("full_process", "failed", f"ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            print(f"\nâŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) > 1:
        project_id = sys.argv[1]
    else:
        project_id = "20260207"

    print(f"Phase 3 í‚¤ì›Œë“œ ë°€ë„ ë° ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ í…ŒìŠ¤íŠ¸ - í”„ë¡œì íŠ¸ ID: {project_id}")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = KeywordAnalysisTester(project_id)
    tester.run_full_analysis()


if __name__ == "__main__":
    main()